{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21c14e6-0804-4eb8-87fc-38f29ff47603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35efc23b-cdc8-4621-a8d0-d35500fe13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.beta import Beta\n",
    "\n",
    "class MyMixUp(Callback):\n",
    "    \"A handler class for implementing `MixUp` style scheduling\"\n",
    "    run_valid = False\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_train(self):\n",
    "        self.stack_y = getattr(self.learn.loss_func, 'y_int', False)\n",
    "        print(f'self.stack_y: {self.stack_y}')\n",
    "        if self.stack_y: self.old_lf,self.learn.loss_func = self.learn.loss_func,self.lf\n",
    "        print(f'self.old_lf: {self.old_lf}, self.loss_func: {self.lf}')\n",
    "\n",
    "    def after_train(self):\n",
    "        if self.stack_y: self.learn.loss_func = self.old_lf\n",
    "\n",
    "    def after_cancel_train(self):\n",
    "        self.after_train()\n",
    "\n",
    "    def before_batch(self):\n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.x.device)\n",
    "        print(f'self.distrib.sample(): {lam.shape}')\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        print(f'stack_lam: {lam.shape}')\n",
    "        self.lam = lam.max(1)[0]\n",
    "        print(f'self.lam: {lam.shape}')\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.x.device)\n",
    "        xb1,self.yb1 = tuple(L(self.xb).itemgot(shuffle)),tuple(L(self.yb).itemgot(shuffle))\n",
    "        print(f'xb1[0].shape: {xb1[0].shape}')\n",
    "        print(f'self.yb1[0].shape: {self.yb1[0].shape}')\n",
    "        nx_dims = len(self.x.size())\n",
    "        print(f'nx_dims: {nx_dims}')\n",
    "        self.learn.xb = tuple(L(xb1,self.xb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=nx_dims-1)))\n",
    "        print(f'self.learn.xb[0].shape: {self.learn.xb[0].shape}')\n",
    "\n",
    "        if not self.stack_y:\n",
    "            ny_dims = len(self.y.size())\n",
    "            print(f'not self stack: nx_dims: {ny_dims}')\n",
    "            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))\n",
    "            print(f'not self stack: self.learn.yb[0].shape: {self.learn.yb[0].shape}, self.learn.yb[0].shape: {self.learn.yb[0].shape}')\n",
    "        print(f'self.learn.yb[0]: {self.learn.yb[0]}')\n",
    "\n",
    "    def lf(self, pred, *yb):\n",
    "        if not self.training: return self.old_lf(pred, *yb)\n",
    "        print(f'pred.shape: {pred.shape}, len(*yb): {len(*yb)}, yb[0].shape: {yb[0].shape}')\n",
    "        print(f'self.lam: {self.lam}')\n",
    "        with NoneReduce(self.old_lf) as lf:\n",
    "            loss = torch.lerp(lf(pred,*self.yb1), lf(pred,*yb), self.lam)\n",
    "        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21d116c-bf36-4c39-9711-8d575c93a673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.stack_y: True\n",
      "self.old_lf: FlattenedLoss of CrossEntropyLoss(), self.loss_func: <bound method MyMixUp.lf of MyMixUp>\n",
      "self.distrib.sample(): torch.Size([64])\n",
      "stack_lam: torch.Size([64, 2])\n",
      "self.lam: torch.Size([64, 2])\n",
      "xb1[0].shape: torch.Size([64, 3, 28, 28])\n",
      "self.yb1[0].shape: torch.Size([64])\n",
      "nx_dims: 4\n",
      "self.learn.xb[0].shape: torch.Size([64, 3, 28, 28])\n",
      "self.learn.yb[0]: TensorCategory([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "pred.shape: torch.Size([64, 1]), len(*yb): 64, yb[0].shape: torch.Size([64])\n",
      "self.lam: tensor([0.9920, 0.7631, 0.8199, 0.7906, 0.7299, 0.9589, 0.6837, 0.9791, 0.7160, 0.9928, 0.6624, 0.9990, 0.9985, 0.9997, 0.8074, 0.9735, 0.9672, 0.9406, 0.8462, 0.5659, 0.9381, 0.7135, 0.8466, 0.9360,\n",
      "        0.9871, 0.6665, 0.6361, 0.6362, 0.7857, 0.9867, 0.7272, 0.8160, 0.9995, 0.9917, 0.7767, 0.9858, 0.7439, 0.7536, 0.9977, 0.9633, 0.5363, 0.8024, 0.7902, 0.9777, 0.8650, 0.6886, 0.6304, 0.6231,\n",
      "        0.6835, 0.9677, 0.7860, 0.8062, 0.9403, 0.9340, 0.7943, 0.6950, 0.5638, 0.9227, 0.8531, 0.7232, 0.9993, 0.8701, 0.9906, 0.9996])\n",
      "self.distrib.sample(): torch.Size([64])\n",
      "stack_lam: torch.Size([64, 2])\n",
      "self.lam: torch.Size([64, 2])\n",
      "xb1[0].shape: torch.Size([64, 3, 28, 28])\n",
      "self.yb1[0].shape: torch.Size([64])\n",
      "nx_dims: 4\n",
      "self.learn.xb[0].shape: torch.Size([64, 3, 28, 28])\n",
      "self.learn.yb[0]: TensorCategory([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "pred.shape: torch.Size([64, 1]), len(*yb): 64, yb[0].shape: torch.Size([64])\n",
      "self.lam: tensor([0.9932, 0.7421, 0.9995, 0.8138, 0.6136, 0.9209, 0.6057, 0.6938, 0.5728, 0.9604, 0.5765, 0.6823, 0.8987, 0.9909, 0.5141, 0.9392, 0.5597, 0.9999, 0.9167, 0.9959, 0.5398, 0.5195, 0.8672, 0.9978,\n",
      "        0.6629, 0.6255, 0.6005, 0.8419, 0.9953, 0.7760, 0.8071, 0.7033, 0.6708, 0.7235, 0.9976, 0.9689, 0.9348, 0.9514, 0.6359, 0.5792, 0.8036, 0.5114, 0.9694, 0.9663, 0.5402, 0.5881, 0.9752, 0.9046,\n",
      "        0.9843, 0.9943, 0.8261, 0.9536, 0.8608, 0.7263, 0.6399, 0.5976, 0.8776, 0.7165, 0.9607, 0.6431, 0.7430, 0.7075, 0.5550, 0.9441])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.stack_y: True\n",
      "self.old_lf: FlattenedLoss of CrossEntropyLoss(), self.loss_func: <bound method MyMixUp.lf of MyMixUp>\n",
      "self.distrib.sample(): torch.Size([64])\n",
      "stack_lam: torch.Size([64, 2])\n",
      "self.lam: torch.Size([64, 2])\n",
      "xb1[0].shape: torch.Size([64, 3, 28, 28])\n",
      "self.yb1[0].shape: torch.Size([64])\n",
      "nx_dims: 4\n",
      "self.learn.xb[0].shape: torch.Size([64, 3, 28, 28])\n",
      "self.learn.yb[0]: TensorCategory([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "pred.shape: torch.Size([64, 1]), len(*yb): 64, yb[0].shape: torch.Size([64])\n",
      "self.lam: tensor([0.8526, 0.9077, 0.9991, 0.9083, 0.5336, 0.6136, 0.6970, 0.5269, 0.5187, 0.6651, 0.7535, 0.9840, 0.9260, 0.7264, 0.8301, 0.9078, 0.9173, 0.9974, 0.7876, 0.8584, 0.5664, 0.7352, 0.6590, 0.6829,\n",
      "        0.8929, 0.9883, 0.8888, 0.8614, 0.7744, 0.7084, 0.8881, 0.9180, 0.5758, 0.5804, 0.5970, 0.8703, 0.9917, 0.7355, 0.8333, 0.6651, 0.9783, 0.6622, 0.9984, 0.9691, 0.7176, 0.9450, 0.9992, 0.8568,\n",
      "        0.9115, 0.7000, 0.9546, 0.8045, 0.6954, 0.8924, 0.9859, 0.9356, 0.9389, 0.9884, 0.9144, 0.9076, 0.7349, 0.9416, 0.8012, 0.6662])\n",
      "self.distrib.sample(): torch.Size([64])\n",
      "stack_lam: torch.Size([64, 2])\n",
      "self.lam: torch.Size([64, 2])\n",
      "xb1[0].shape: torch.Size([64, 3, 28, 28])\n",
      "self.yb1[0].shape: torch.Size([64])\n",
      "nx_dims: 4\n",
      "self.learn.xb[0].shape: torch.Size([64, 3, 28, 28])\n",
      "self.learn.yb[0]: TensorCategory([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "pred.shape: torch.Size([64, 1]), len(*yb): 64, yb[0].shape: torch.Size([64])\n",
      "self.lam: tensor([0.8339, 0.9566, 0.7535, 0.9603, 0.6552, 0.9979, 0.9583, 0.6165, 0.8468, 0.9999, 0.8063, 0.9668, 0.7165, 0.7267, 0.5393, 0.8938, 0.9875, 0.9928, 0.7948, 0.5857, 0.9670, 0.8779, 0.8638, 0.7957,\n",
      "        0.9360, 1.0000, 0.9999, 1.0000, 0.6487, 0.9619, 0.8155, 0.7348, 0.9985, 0.8986, 0.9996, 0.8251, 0.7308, 0.9972, 0.9574, 0.7916, 0.9020, 0.5688, 0.7008, 0.8136, 0.9565, 0.9911, 0.6947, 0.8770,\n",
      "        0.5523, 0.6324, 0.5192, 0.9109, 0.9961, 0.5240, 0.9978, 0.5808, 0.8064, 0.9804, 0.6881, 0.7371, 0.9967, 0.9879, 0.8761, 0.9267])\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "#path = untar_data(URLs.PETS)/'images'\n",
    "path = Path('/home/pawel/Pictures/oxford-iiit-pet/')\n",
    "\n",
    "def is_cat(x): return x[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(28))\n",
    "\n",
    "learn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), metrics=error_rate, cbs=MyMixUp())\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d963d13e-bad8-4731-870c-e6f1d3046020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7393) [Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/Bombay_160.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/basset_hound_124.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_144.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/basset_hound_95.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/american_bulldog_66.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/pug_27.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/wheaten_terrier_95.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/basset_hound_162.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/Maine_Coon_104.jpg'),Path('/home/pawel/.fastai/data/oxford-iiit-pet/images/pomeranian_15.jpg')...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38f219-a6fa-4d8a-8bf7-73dc5502c40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
